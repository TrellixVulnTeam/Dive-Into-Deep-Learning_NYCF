{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.13 丢弃法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "除了前一节介绍的权重衰减以外，深度学习模型常常使用丢弃法(dropout)来应对过拟合问题。丢弃法有一些不同的变体。本节中提到的丢弃法特指倒置丢弃法(inverted dropout)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.13.1 方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "回忆一下，[“多层感知机”](mlp.ipynb)一节的图3.3描述了一个单隐藏层的多层感知机。其中输入个数为4，隐藏层单元个数为5，且隐藏单元$h_i(i=1,...,5)$的计算表达式为"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 这里$\\phi$是激活函数，$x_1, \\ldots, x_4$是输入，隐藏单元$i$的权重参数为$w_{1i}, \\ldots, w_{4i}$，偏差参数为$b_i$。当对该隐藏层使用丢弃法时，该层的隐藏单元将有一定概率被丢弃掉。设丢弃概率为$p$，那么有$p$的概率$h_i$会被清零，有$1-p$的概率$h_i$会除以$1-p$做拉伸。丢弃概率是丢弃法的超参数。具体来说，设随机变量$\\xi_i$为0和1的概率分别为$p$和$1-p$。使用丢弃法时我们计算新的隐藏单元$h_i'$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$h_i' = \\frac{\\xi_i}{1-p} h_i.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于$E(\\xi_i) = 1-p$，因此"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$E(h_i') = \\frac{E(\\xi_i)}{1-p}h_i = h_i.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "即丢弃法不改变其输入的期望值。让我们对图3.3中的隐藏层使用丢弃法，一种可能的结果如图3.5所示，其中$h_2$和$h_5$被清零。这时输出值的计算不再依赖$h_2$和$h_5$，在反向传播时，与这两个隐藏单元相关的权重的梯度均为0。由于在训练中隐藏层神经元的丢弃是随机的，即$h_1, \\ldots, h_5$都有可能被清零，输出层的计算无法过度依赖$h_1, \\ldots, h_5$中的任一个，从而在训练模型时起到正则化的作用，并可以用来应对过拟合。在测试模型时，我们为了得到更加确定性的结果，一般不使用丢弃法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![隐藏层使用了丢弃法的多层感知机](../img/dropout.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.13.2 从零开始实现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "根据丢弃法的定义，我们可以很容易地实现它。下面的dropout函数将以drop_prob的概率丢弃NDArray输入X中的元素。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import d2lzh as d2l\n",
    "from mxnet import autograd, gluon, init, nd\n",
    "from mxnet.gluon import loss as gloss, nn\n",
    "\n",
    "def dropout(X, drop_prob):\n",
    "    assert 0 <= drop_prob <= 1\n",
    "    keep_prob = 1 - drop_prob\n",
    "    # 这种情况下把全部元素都丢弃\n",
    "    if keep_prob == 0:\n",
    "        return X.zeros_like()\n",
    "    mask = nd.random_uniform(0, 1, X.shape) < keep_prob\n",
    "    return mask * X / keep_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们运行几个例子来测试一下`dropout`函数。其中丢弃概率分别为0、0.5和1。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[ 0.  1.  2.  3.  4.  5.  6.  7.]\n",
       " [ 8.  9. 10. 11. 12. 13. 14. 15.]]\n",
       "<NDArray 2x8 @cpu(0)>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = nd.arange(16).reshape((2, 8))\n",
    "dropout(X, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[ 0.  2.  0.  6.  0. 10. 12. 14.]\n",
       " [16.  0.  0.  0. 24.  0. 28.  0.]]\n",
       "<NDArray 2x8 @cpu(0)>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dropout(X, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[0. 0. 0. 0. 0. 0. 0. 0.]\n",
       " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
       "<NDArray 2x8 @cpu(0)>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dropout(X, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 定义模型参数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实验中，我们依然使用[“softmax回归的从零开始实现”](softmax-regression-scratch.ipynb)一节中介绍的Fashion-MNIST数据集。我们将定义一个包含两个隐藏层的多层感知机，其中两个隐藏层的输出个数都是256。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inputs, num_outputs, num_hiddens1, num_hiddens2 = 784, 10, 256, 256\n",
    "\n",
    "W1 = nd.random_normal(scale=0.01, shape=(num_inputs, num_hiddens1))\n",
    "b1 = nd.zeros(num_hiddens1)\n",
    "W2 = nd.random_normal(scale=0.01, shape=(num_hiddens1, num_hiddens2))\n",
    "b2 = nd.zeros(num_hiddens2)\n",
    "W3 = nd.random_normal(scale=0.01, shape=(num_hiddens2, num_outputs))\n",
    "b3 = nd.zeros(num_outputs)\n",
    "\n",
    "params = [W1, b1, W2, b2, W3, b3]\n",
    "for param in params:\n",
    "    param.attach_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 定义模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_prob1, drop_prob2 = 0.2, 0.5\n",
    "\n",
    "def net(X):\n",
    "    X = X.reshape((-1, num_inputs))\n",
    "    H1 = (nd.dot(X, W1) + b1).relu()\n",
    "    if autograd.is_training(): # 只在训练模式时使用丢弃法\n",
    "        H1 = dropout(H1, drop_prob1) # 在第一层全连接层后添加丢弃层\n",
    "    H2 = (nd.dot(H1, W2) + b2).relu()\n",
    "    if autograd.is_training():\n",
    "        H2 = dropout(H2, drop_prob2) # 在第二层全连接层后添加丢弃层\n",
    "    return nd.dot(H2, W3) + b3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 训练和测试模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这部分与之前多层感知机的训练和测试类似。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 1.1398, train acc 0.557, test acc 0.768\n",
      "epoch 2, loss 0.6074, train acc 0.773, test acc 0.826\n",
      "epoch 3, loss 0.5094, train acc 0.812, test acc 0.842\n",
      "epoch 4, loss 0.4603, train acc 0.833, test acc 0.838\n",
      "epoch 5, loss 0.4275, train acc 0.844, test acc 0.860\n",
      "epoch 6, loss 0.4060, train acc 0.852, test acc 0.856\n",
      "epoch 7, loss 0.3906, train acc 0.858, test acc 0.862\n",
      "epoch 8, loss 0.3726, train acc 0.865, test acc 0.860\n",
      "epoch 9, loss 0.3673, train acc 0.868, test acc 0.873\n",
      "epoch 10, loss 0.3552, train acc 0.871, test acc 0.877\n",
      "epoch 11, loss 0.3465, train acc 0.873, test acc 0.871\n",
      "epoch 12, loss 0.3398, train acc 0.875, test acc 0.882\n",
      "epoch 13, loss 0.3289, train acc 0.879, test acc 0.883\n",
      "epoch 14, loss 0.3243, train acc 0.881, test acc 0.884\n",
      "epoch 15, loss 0.3176, train acc 0.883, test acc 0.882\n",
      "epoch 16, loss 0.3102, train acc 0.885, test acc 0.873\n",
      "epoch 17, loss 0.3039, train acc 0.887, test acc 0.877\n",
      "epoch 18, loss 0.2996, train acc 0.888, test acc 0.883\n",
      "epoch 19, loss 0.2996, train acc 0.888, test acc 0.891\n",
      "epoch 20, loss 0.2920, train acc 0.891, test acc 0.891\n",
      "epoch 21, loss 0.2894, train acc 0.891, test acc 0.890\n",
      "epoch 22, loss 0.2828, train acc 0.894, test acc 0.888\n",
      "epoch 23, loss 0.2818, train acc 0.895, test acc 0.891\n",
      "epoch 24, loss 0.2773, train acc 0.897, test acc 0.891\n",
      "epoch 25, loss 0.2731, train acc 0.898, test acc 0.889\n",
      "epoch 26, loss 0.2708, train acc 0.899, test acc 0.889\n",
      "epoch 27, loss 0.2694, train acc 0.900, test acc 0.891\n",
      "epoch 28, loss 0.2656, train acc 0.900, test acc 0.889\n",
      "epoch 29, loss 0.2637, train acc 0.901, test acc 0.893\n",
      "epoch 30, loss 0.2613, train acc 0.902, test acc 0.888\n"
     ]
    }
   ],
   "source": [
    "num_epochs, lr, batch_size = 30, 0.5, 256\n",
    "loss = gloss.SoftmaxCrossEntropyLoss()\n",
    "train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)\n",
    "d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size,\n",
    "             params, lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.13.3 简洁实现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在Gluon中，我们只需要在全连接层后添加`Dropout`层并指定丢弃概率。在训练模型时，`Dropout`层将以指定的丢弃概率随机丢弃上一层的输出元素；在测试模型时，`Dropout`层并不发挥作用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.Sequential()\n",
    "net.add(nn.Dense(256, activation=\"relu\"),\n",
    "       nn.Dropout(drop_prob1), # 在第一个全连接层后添加丢弃层\n",
    "       nn.Dense(256, activation=\"relu\"),\n",
    "       nn.Dropout(drop_prob2), # 在第二个全连接层后添加丢弃层\n",
    "       nn.Dense(10))\n",
    "net.initialize(init.Normal(sigma=0.01))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面训练并测试模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 1.2124, train acc 0.531, test acc 0.768\n",
      "epoch 2, loss 0.6007, train acc 0.777, test acc 0.832\n",
      "epoch 3, loss 0.5024, train acc 0.816, test acc 0.848\n",
      "epoch 4, loss 0.4574, train acc 0.833, test acc 0.861\n",
      "epoch 5, loss 0.4265, train acc 0.844, test acc 0.856\n",
      "epoch 6, loss 0.4060, train acc 0.853, test acc 0.862\n",
      "epoch 7, loss 0.3982, train acc 0.856, test acc 0.861\n",
      "epoch 8, loss 0.3799, train acc 0.861, test acc 0.875\n",
      "epoch 9, loss 0.3665, train acc 0.865, test acc 0.870\n",
      "epoch 10, loss 0.3567, train acc 0.870, test acc 0.869\n",
      "epoch 11, loss 0.3487, train acc 0.873, test acc 0.881\n",
      "epoch 12, loss 0.3379, train acc 0.877, test acc 0.878\n",
      "epoch 13, loss 0.3380, train acc 0.875, test acc 0.882\n",
      "epoch 14, loss 0.3282, train acc 0.879, test acc 0.873\n",
      "epoch 15, loss 0.3210, train acc 0.882, test acc 0.871\n",
      "epoch 16, loss 0.3142, train acc 0.884, test acc 0.883\n",
      "epoch 17, loss 0.3126, train acc 0.885, test acc 0.886\n",
      "epoch 18, loss 0.3062, train acc 0.887, test acc 0.888\n",
      "epoch 19, loss 0.3011, train acc 0.890, test acc 0.886\n",
      "epoch 20, loss 0.2972, train acc 0.890, test acc 0.889\n",
      "epoch 21, loss 0.2927, train acc 0.891, test acc 0.891\n",
      "epoch 22, loss 0.2895, train acc 0.893, test acc 0.891\n",
      "epoch 23, loss 0.2846, train acc 0.894, test acc 0.891\n",
      "epoch 24, loss 0.2823, train acc 0.895, test acc 0.887\n",
      "epoch 25, loss 0.2798, train acc 0.895, test acc 0.893\n",
      "epoch 26, loss 0.2755, train acc 0.897, test acc 0.894\n",
      "epoch 27, loss 0.2748, train acc 0.897, test acc 0.892\n",
      "epoch 28, loss 0.2686, train acc 0.900, test acc 0.886\n",
      "epoch 29, loss 0.2677, train acc 0.900, test acc 0.888\n",
      "epoch 30, loss 0.2636, train acc 0.902, test acc 0.894\n"
     ]
    }
   ],
   "source": [
    "trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': lr})\n",
    "d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size, None,\n",
    "             None, trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 小结"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 我们可以通过使用丢弃法应对过拟合。\n",
    "- 丢弃法只在训练模型时使用。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 练习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 如果把本节中的两个丢弃概率超参数对调，会有什么结果？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_prob1, drop_prob2 = 0.5, 0.2\n",
    "\n",
    "net = nn.Sequential()\n",
    "net.add(nn.Dense(256, activation=\"relu\"),\n",
    "       nn.Dropout(drop_prob1), # 在第一个全连接层后添加丢弃层\n",
    "       nn.Dense(256, activation=\"relu\"),\n",
    "       nn.Dropout(drop_prob2), # 在第二个全连接层后添加丢弃层\n",
    "       nn.Dense(10))\n",
    "net.initialize(init.Normal(sigma=0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 1.1171, train acc 0.563, test acc 0.771\n",
      "epoch 2, loss 0.5877, train acc 0.780, test acc 0.832\n",
      "epoch 3, loss 0.5078, train acc 0.812, test acc 0.845\n",
      "epoch 4, loss 0.4690, train acc 0.827, test acc 0.854\n",
      "epoch 5, loss 0.4422, train acc 0.838, test acc 0.856\n",
      "epoch 6, loss 0.4231, train acc 0.844, test acc 0.863\n",
      "epoch 7, loss 0.4049, train acc 0.851, test acc 0.866\n",
      "epoch 8, loss 0.3971, train acc 0.855, test acc 0.868\n",
      "epoch 9, loss 0.3834, train acc 0.859, test acc 0.866\n",
      "epoch 10, loss 0.3775, train acc 0.860, test acc 0.876\n",
      "epoch 11, loss 0.3659, train acc 0.864, test acc 0.874\n",
      "epoch 12, loss 0.3592, train acc 0.867, test acc 0.877\n",
      "epoch 13, loss 0.3568, train acc 0.868, test acc 0.876\n",
      "epoch 14, loss 0.3477, train acc 0.872, test acc 0.880\n",
      "epoch 15, loss 0.3464, train acc 0.871, test acc 0.883\n",
      "epoch 16, loss 0.3375, train acc 0.875, test acc 0.882\n",
      "epoch 17, loss 0.3341, train acc 0.877, test acc 0.881\n",
      "epoch 18, loss 0.3312, train acc 0.877, test acc 0.880\n",
      "epoch 19, loss 0.3271, train acc 0.879, test acc 0.884\n",
      "epoch 20, loss 0.3234, train acc 0.881, test acc 0.882\n",
      "epoch 21, loss 0.3212, train acc 0.881, test acc 0.887\n",
      "epoch 22, loss 0.3149, train acc 0.883, test acc 0.883\n",
      "epoch 23, loss 0.3146, train acc 0.884, test acc 0.890\n",
      "epoch 24, loss 0.3093, train acc 0.886, test acc 0.888\n",
      "epoch 25, loss 0.3088, train acc 0.887, test acc 0.886\n",
      "epoch 26, loss 0.3028, train acc 0.888, test acc 0.884\n",
      "epoch 27, loss 0.3031, train acc 0.886, test acc 0.887\n",
      "epoch 28, loss 0.2987, train acc 0.888, test acc 0.887\n",
      "epoch 29, loss 0.2935, train acc 0.890, test acc 0.891\n",
      "epoch 30, loss 0.2941, train acc 0.890, test acc 0.895\n"
     ]
    }
   ],
   "source": [
    "trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': lr})\n",
    "d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size, None,\n",
    "             None, trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 增大迭代周期数，比较使用丢弃法与不使用丢弃法的结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用丢弃法\n",
    "num_epochs = 40\n",
    "drop_prob1, drop_prob2 = 0.2, 0.5\n",
    "\n",
    "net = nn.Sequential()\n",
    "net.add(nn.Dense(256, activation=\"relu\"),\n",
    "       nn.Dropout(drop_prob1), # 在第一个全连接层后添加丢弃层\n",
    "       nn.Dense(256, activation=\"relu\"),\n",
    "       nn.Dropout(drop_prob2), # 在第二个全连接层后添加丢弃层\n",
    "       nn.Dense(10))\n",
    "net.initialize(init.Normal(sigma=0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 1.1370, train acc 0.563, test acc 0.775\n",
      "epoch 2, loss 0.5851, train acc 0.781, test acc 0.839\n",
      "epoch 3, loss 0.4923, train acc 0.819, test acc 0.855\n",
      "epoch 4, loss 0.4484, train acc 0.836, test acc 0.833\n",
      "epoch 5, loss 0.4230, train acc 0.846, test acc 0.864\n",
      "epoch 6, loss 0.3966, train acc 0.856, test acc 0.868\n",
      "epoch 7, loss 0.3843, train acc 0.858, test acc 0.869\n",
      "epoch 8, loss 0.3701, train acc 0.865, test acc 0.870\n",
      "epoch 9, loss 0.3554, train acc 0.871, test acc 0.878\n",
      "epoch 10, loss 0.3509, train acc 0.872, test acc 0.877\n",
      "epoch 11, loss 0.3368, train acc 0.876, test acc 0.880\n",
      "epoch 12, loss 0.3308, train acc 0.878, test acc 0.882\n",
      "epoch 13, loss 0.3258, train acc 0.879, test acc 0.876\n",
      "epoch 14, loss 0.3179, train acc 0.883, test acc 0.880\n",
      "epoch 15, loss 0.3113, train acc 0.886, test acc 0.884\n",
      "epoch 16, loss 0.3032, train acc 0.888, test acc 0.882\n",
      "epoch 17, loss 0.2996, train acc 0.889, test acc 0.887\n",
      "epoch 18, loss 0.2913, train acc 0.892, test acc 0.889\n",
      "epoch 19, loss 0.2921, train acc 0.891, test acc 0.886\n",
      "epoch 20, loss 0.2830, train acc 0.894, test acc 0.882\n",
      "epoch 21, loss 0.2777, train acc 0.896, test acc 0.892\n",
      "epoch 22, loss 0.2778, train acc 0.897, test acc 0.890\n",
      "epoch 23, loss 0.2775, train acc 0.897, test acc 0.889\n",
      "epoch 24, loss 0.2694, train acc 0.899, test acc 0.892\n",
      "epoch 25, loss 0.2679, train acc 0.900, test acc 0.887\n",
      "epoch 26, loss 0.2636, train acc 0.903, test acc 0.897\n",
      "epoch 27, loss 0.2595, train acc 0.903, test acc 0.891\n",
      "epoch 28, loss 0.2594, train acc 0.902, test acc 0.891\n",
      "epoch 29, loss 0.2563, train acc 0.904, test acc 0.894\n",
      "epoch 30, loss 0.2542, train acc 0.905, test acc 0.894\n",
      "epoch 31, loss 0.2498, train acc 0.906, test acc 0.898\n",
      "epoch 32, loss 0.2466, train acc 0.907, test acc 0.890\n",
      "epoch 33, loss 0.2431, train acc 0.908, test acc 0.896\n",
      "epoch 34, loss 0.2432, train acc 0.909, test acc 0.894\n",
      "epoch 35, loss 0.2417, train acc 0.909, test acc 0.890\n",
      "epoch 36, loss 0.2360, train acc 0.912, test acc 0.896\n",
      "epoch 37, loss 0.2392, train acc 0.910, test acc 0.894\n",
      "epoch 38, loss 0.2333, train acc 0.912, test acc 0.897\n",
      "epoch 39, loss 0.2299, train acc 0.914, test acc 0.900\n",
      "epoch 40, loss 0.2281, train acc 0.914, test acc 0.901\n"
     ]
    }
   ],
   "source": [
    "trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': lr})\n",
    "d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size, None,\n",
    "             None, trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 不使用丢弃法\n",
    "num_epochs = 40\n",
    "drop_prob1, drop_prob2 = 0, 0\n",
    "\n",
    "net = nn.Sequential()\n",
    "net.add(nn.Dense(256, activation=\"relu\"),\n",
    "       nn.Dropout(drop_prob1), # 在第一个全连接层后添加丢弃层\n",
    "       nn.Dense(256, activation=\"relu\"),\n",
    "       nn.Dropout(drop_prob2), # 在第二个全连接层后添加丢弃层\n",
    "       nn.Dense(10))\n",
    "net.initialize(init.Normal(sigma=0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 1.1036, train acc 0.565, test acc 0.783\n",
      "epoch 2, loss 0.5718, train acc 0.784, test acc 0.829\n",
      "epoch 3, loss 0.4613, train acc 0.828, test acc 0.844\n",
      "epoch 4, loss 0.4159, train acc 0.845, test acc 0.852\n",
      "epoch 5, loss 0.3806, train acc 0.859, test acc 0.871\n",
      "epoch 6, loss 0.3639, train acc 0.864, test acc 0.867\n",
      "epoch 7, loss 0.3478, train acc 0.871, test acc 0.869\n",
      "epoch 8, loss 0.3345, train acc 0.876, test acc 0.875\n",
      "epoch 9, loss 0.3184, train acc 0.881, test acc 0.880\n",
      "epoch 10, loss 0.3110, train acc 0.883, test acc 0.869\n",
      "epoch 11, loss 0.2988, train acc 0.888, test acc 0.883\n",
      "epoch 12, loss 0.2899, train acc 0.891, test acc 0.886\n",
      "epoch 13, loss 0.2795, train acc 0.895, test acc 0.884\n",
      "epoch 14, loss 0.2745, train acc 0.897, test acc 0.890\n",
      "epoch 15, loss 0.2644, train acc 0.900, test acc 0.886\n",
      "epoch 16, loss 0.2628, train acc 0.901, test acc 0.885\n",
      "epoch 17, loss 0.2505, train acc 0.905, test acc 0.893\n",
      "epoch 18, loss 0.2457, train acc 0.908, test acc 0.885\n",
      "epoch 19, loss 0.2429, train acc 0.908, test acc 0.893\n",
      "epoch 20, loss 10.5934, train acc 0.618, test acc 0.566\n",
      "epoch 21, loss 0.7082, train acc 0.720, test acc 0.775\n",
      "epoch 22, loss 0.5268, train acc 0.794, test acc 0.831\n",
      "epoch 23, loss 0.4916, train acc 0.817, test acc 0.832\n",
      "epoch 24, loss 0.4341, train acc 0.838, test acc 0.851\n",
      "epoch 25, loss 0.4118, train acc 0.846, test acc 0.858\n",
      "epoch 26, loss 0.4355, train acc 0.843, test acc 0.839\n",
      "epoch 27, loss 0.4036, train acc 0.851, test acc 0.865\n",
      "epoch 28, loss 0.3963, train acc 0.854, test acc 0.869\n",
      "epoch 29, loss 0.3536, train acc 0.869, test acc 0.873\n",
      "epoch 30, loss 0.3378, train acc 0.875, test acc 0.870\n",
      "epoch 31, loss 0.3243, train acc 0.879, test acc 0.880\n",
      "epoch 32, loss 0.3173, train acc 0.881, test acc 0.881\n",
      "epoch 33, loss 0.3100, train acc 0.885, test acc 0.881\n",
      "epoch 34, loss 0.3066, train acc 0.886, test acc 0.875\n",
      "epoch 35, loss 0.2997, train acc 0.888, test acc 0.879\n",
      "epoch 36, loss 0.2956, train acc 0.890, test acc 0.882\n",
      "epoch 37, loss 0.2892, train acc 0.892, test acc 0.878\n",
      "epoch 38, loss 0.2854, train acc 0.893, test acc 0.879\n",
      "epoch 39, loss 0.2830, train acc 0.895, test acc 0.878\n",
      "epoch 40, loss 0.2763, train acc 0.897, test acc 0.885\n"
     ]
    }
   ],
   "source": [
    "trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': lr})\n",
    "d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size, None,\n",
    "             None, trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 如果将模型改得更复杂，如增加隐藏层单元，使用丢弃法应对过拟合的效果是否更加明显？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 40\n",
    "drop_prob1, drop_prob2 = 0.2, 0.5\n",
    "\n",
    "net = nn.Sequential()\n",
    "net.add(nn.Dense(512, activation=\"relu\"),\n",
    "       nn.Dropout(drop_prob1), # 在第一个全连接层后添加丢弃层\n",
    "       nn.Dense(512, activation=\"relu\"),\n",
    "       nn.Dropout(drop_prob2), # 在第二个全连接层后添加丢弃层\n",
    "       nn.Dense(10))\n",
    "net.initialize(init.Normal(sigma=0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 1.1726, train acc 0.549, test acc 0.789\n",
      "epoch 2, loss 0.5851, train acc 0.782, test acc 0.824\n",
      "epoch 3, loss 0.4888, train acc 0.819, test acc 0.838\n",
      "epoch 4, loss 0.4400, train acc 0.838, test acc 0.850\n",
      "epoch 5, loss 0.4084, train acc 0.849, test acc 0.865\n",
      "epoch 6, loss 0.3897, train acc 0.857, test acc 0.872\n",
      "epoch 7, loss 0.3723, train acc 0.863, test acc 0.866\n",
      "epoch 8, loss 0.3617, train acc 0.868, test acc 0.876\n",
      "epoch 9, loss 0.3465, train acc 0.873, test acc 0.883\n",
      "epoch 10, loss 0.3370, train acc 0.875, test acc 0.877\n",
      "epoch 11, loss 0.3279, train acc 0.878, test acc 0.884\n",
      "epoch 12, loss 0.3208, train acc 0.881, test acc 0.873\n",
      "epoch 13, loss 0.3131, train acc 0.884, test acc 0.884\n",
      "epoch 14, loss 0.3059, train acc 0.885, test acc 0.883\n",
      "epoch 15, loss 0.3011, train acc 0.888, test acc 0.886\n",
      "epoch 16, loss 0.2938, train acc 0.890, test acc 0.887\n",
      "epoch 17, loss 0.2909, train acc 0.890, test acc 0.888\n",
      "epoch 18, loss 0.2849, train acc 0.894, test acc 0.885\n",
      "epoch 19, loss 0.2838, train acc 0.895, test acc 0.889\n",
      "epoch 20, loss 0.2760, train acc 0.897, test acc 0.891\n",
      "epoch 21, loss 0.2735, train acc 0.898, test acc 0.890\n",
      "epoch 22, loss 0.2669, train acc 0.899, test acc 0.885\n",
      "epoch 23, loss 0.2658, train acc 0.901, test acc 0.891\n",
      "epoch 24, loss 0.2617, train acc 0.902, test acc 0.895\n",
      "epoch 25, loss 0.2594, train acc 0.903, test acc 0.893\n",
      "epoch 26, loss 0.2552, train acc 0.904, test acc 0.891\n",
      "epoch 27, loss 0.2492, train acc 0.907, test acc 0.895\n",
      "epoch 28, loss 0.2466, train acc 0.907, test acc 0.893\n",
      "epoch 29, loss 0.2459, train acc 0.907, test acc 0.896\n",
      "epoch 30, loss 0.2431, train acc 0.909, test acc 0.893\n",
      "epoch 31, loss 0.2409, train acc 0.909, test acc 0.894\n",
      "epoch 32, loss 0.2370, train acc 0.911, test acc 0.893\n",
      "epoch 33, loss 0.2347, train acc 0.911, test acc 0.889\n",
      "epoch 34, loss 0.2306, train acc 0.913, test acc 0.896\n",
      "epoch 35, loss 0.2276, train acc 0.913, test acc 0.901\n",
      "epoch 36, loss 0.2247, train acc 0.915, test acc 0.898\n",
      "epoch 37, loss 0.2227, train acc 0.915, test acc 0.896\n",
      "epoch 38, loss 0.2244, train acc 0.914, test acc 0.899\n",
      "epoch 39, loss 0.2207, train acc 0.917, test acc 0.898\n",
      "epoch 40, loss 0.2181, train acc 0.917, test acc 0.898\n"
     ]
    }
   ],
   "source": [
    "trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': lr})\n",
    "d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size, None,\n",
    "             None, trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 以本节中的模型为例，比较使用丢弃法与权重衰减的效果。如果同时使用丢弃法和权重衰减，效果会如何？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "丢弃法效果比权重衰减效果更好。同时使用丢弃法和权重衰减并不理想，还不如单独使用丢弃法或者权重衰减。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
